{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0a8d8e-504f-450a-a23f-878ef5d878a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import inspect\n",
    "import random\n",
    "import pickle\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "import tiktoken\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import pipeline,set_seed\n",
    "from os.path import exists\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True\n",
    "\n",
    "# 设置打印选项，禁用科学计数法\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "# 设置打印选项，取消显示省略号\n",
    "torch.set_printoptions(threshold=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b71d3f5-29a7-4ae0-b7e3-057f8f658473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取系统中可用的 CUDA 设备（即 GPU）的数量\n",
    "torch.cuda.device_count()\n",
    "\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "298df7da-166c-4f29-b2d9-2488dd9436f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BicubicLayer(nn.Module):\n",
    "    def __init__(self, size=None, scale_factor=None, in_channels=None, out_channels=None,\n",
    "                 kernel_size=None, padding=None, stride=None):\n",
    "        super(BicubicLayer, self).__init__()\n",
    "        \n",
    "        self.size = size\n",
    "        self.scale_factor = scale_factor\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                              kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        \n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = F.interpolate(X, size=self.size, scale_factor=self.scale_factor,mode='bicubic', align_corners=False)\n",
    "        Y = self.conv(Y)\n",
    "        Y = self.elu(Y)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "276a5db0-a9f7-4101-b9b9-ac2d009411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_1 = BicubicLayer(size=(16, 16), in_channels=2, out_channels=8, kernel_size=2, padding=(0, 0), stride=(1,1))\n",
    "net_2 = BicubicLayer(size=(32, 32), in_channels=8, out_channels=16, kernel_size=2, padding=(0, 0), stride=1)\n",
    "net_3 = BicubicLayer(size=(72, 72), in_channels=16, out_channels=32, kernel_size=3, padding=(1, 1), stride=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "165af7cd-1fc0-49db-b2ce-75afb6964e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立整体网络结构\n",
    "net_block_1 = nn.Sequential(\n",
    "    net_1,\n",
    "    net_2,\n",
    "    net_3,).to(try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "792a63bc-6e6b-475e-a84b-2d40f8cfa102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 15, 15]              72\n",
      "               ELU-2            [-1, 8, 15, 15]               0\n",
      "      BicubicLayer-3            [-1, 8, 15, 15]               0\n",
      "            Conv2d-4           [-1, 16, 31, 31]             528\n",
      "               ELU-5           [-1, 16, 31, 31]               0\n",
      "      BicubicLayer-6           [-1, 16, 31, 31]               0\n",
      "            Conv2d-7           [-1, 32, 72, 72]           4,640\n",
      "               ELU-8           [-1, 32, 72, 72]               0\n",
      "      BicubicLayer-9           [-1, 32, 72, 72]               0\n",
      "================================================================\n",
      "Total params: 5,240\n",
      "Trainable params: 5,240\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 4.19\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 4.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net_block_1, input_size=(2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "026a671e-0477-411a-8693-3308dee5716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):  \n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides) #高宽不变\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y =  F.relu(self.bn1(self.conv1(X)))  \n",
    "        Y = self.bn2(self.conv2(Y))         \n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)                \n",
    "        Y += X\n",
    "        return F.relu(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d26617b7-9061-44ba-bf07-208188cc3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_4 = Residual(32,48,use_1x1conv=True)\n",
    "net_5 = Residual(48,56,use_1x1conv=True)\n",
    "net_6 = Residual(56,64,use_1x1conv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5e1fc10-bd73-46f7-bbc0-7dfb5019306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立整体网络结构\n",
    "net_block_2 = nn.Sequential(\n",
    "    net_1,\n",
    "    net_2,\n",
    "    net_3,\n",
    "    net_4,\n",
    "    net_5,\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "    net_6,\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "    ).to(try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ae29a85-1aa4-49b4-81c5-ca8db7b3d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 15, 15]              72\n",
      "               ELU-2            [-1, 8, 15, 15]               0\n",
      "      BicubicLayer-3            [-1, 8, 15, 15]               0\n",
      "            Conv2d-4           [-1, 16, 31, 31]             528\n",
      "               ELU-5           [-1, 16, 31, 31]               0\n",
      "      BicubicLayer-6           [-1, 16, 31, 31]               0\n",
      "            Conv2d-7           [-1, 32, 72, 72]           4,640\n",
      "               ELU-8           [-1, 32, 72, 72]               0\n",
      "      BicubicLayer-9           [-1, 32, 72, 72]               0\n",
      "           Conv2d-10           [-1, 48, 72, 72]          13,872\n",
      "      BatchNorm2d-11           [-1, 48, 72, 72]              96\n",
      "           Conv2d-12           [-1, 48, 72, 72]          20,784\n",
      "      BatchNorm2d-13           [-1, 48, 72, 72]              96\n",
      "           Conv2d-14           [-1, 48, 72, 72]           1,584\n",
      "         Residual-15           [-1, 48, 72, 72]               0\n",
      "           Conv2d-16           [-1, 56, 72, 72]          24,248\n",
      "      BatchNorm2d-17           [-1, 56, 72, 72]             112\n",
      "           Conv2d-18           [-1, 56, 72, 72]          28,280\n",
      "      BatchNorm2d-19           [-1, 56, 72, 72]             112\n",
      "           Conv2d-20           [-1, 56, 72, 72]           2,744\n",
      "         Residual-21           [-1, 56, 72, 72]               0\n",
      "        MaxPool2d-22           [-1, 56, 36, 36]               0\n",
      "           Conv2d-23           [-1, 64, 36, 36]          32,320\n",
      "      BatchNorm2d-24           [-1, 64, 36, 36]             128\n",
      "           Conv2d-25           [-1, 64, 36, 36]          36,928\n",
      "      BatchNorm2d-26           [-1, 64, 36, 36]             128\n",
      "           Conv2d-27           [-1, 64, 36, 36]           3,648\n",
      "         Residual-28           [-1, 64, 36, 36]               0\n",
      "        MaxPool2d-29           [-1, 64, 18, 18]               0\n",
      "================================================================\n",
      "Total params: 170,320\n",
      "Trainable params: 170,320\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 33.38\n",
      "Params size (MB): 0.65\n",
      "Estimated Total Size (MB): 34.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net_block_2, input_size=(2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ba6bc91-24b6-4b9c-a82e-ce4efdd492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten_Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten_Module, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(64 * 18 * 18, 400)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        flattened = self.flatten(x)  # Shape will be [batch_size, 64 * 9 * 9]\n",
    "        linear_output = self.linear(flattened)  # Shape will be [batch_size, 1200]\n",
    "        \n",
    "        # Reshape to desired shape [batch_size, 20, 60]\n",
    "        reshaped = linear_output.view( -1,20, 20)\n",
    "        \n",
    "        return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c94ad79-656d-4713-ad56-c8678c24727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_7 = Flatten_Module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c59ce944-119d-4433-a90e-e8fa68d3f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立整体网络结构\n",
    "net = nn.Sequential(\n",
    "    net_1,\n",
    "    net_2,\n",
    "    net_3,\n",
    "    net_4,\n",
    "    net_5,\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "    net_6,\n",
    "    nn.MaxPool2d(kernel_size=2,stride=2,padding=0),\n",
    "    net_7\n",
    "    ).to(try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc930550-57e5-4565-9e54-224f71af8595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 15, 15]              72\n",
      "               ELU-2            [-1, 8, 15, 15]               0\n",
      "      BicubicLayer-3            [-1, 8, 15, 15]               0\n",
      "            Conv2d-4           [-1, 16, 31, 31]             528\n",
      "               ELU-5           [-1, 16, 31, 31]               0\n",
      "      BicubicLayer-6           [-1, 16, 31, 31]               0\n",
      "            Conv2d-7           [-1, 32, 72, 72]           4,640\n",
      "               ELU-8           [-1, 32, 72, 72]               0\n",
      "      BicubicLayer-9           [-1, 32, 72, 72]               0\n",
      "           Conv2d-10           [-1, 48, 72, 72]          13,872\n",
      "      BatchNorm2d-11           [-1, 48, 72, 72]              96\n",
      "           Conv2d-12           [-1, 48, 72, 72]          20,784\n",
      "      BatchNorm2d-13           [-1, 48, 72, 72]              96\n",
      "           Conv2d-14           [-1, 48, 72, 72]           1,584\n",
      "         Residual-15           [-1, 48, 72, 72]               0\n",
      "           Conv2d-16           [-1, 56, 72, 72]          24,248\n",
      "      BatchNorm2d-17           [-1, 56, 72, 72]             112\n",
      "           Conv2d-18           [-1, 56, 72, 72]          28,280\n",
      "      BatchNorm2d-19           [-1, 56, 72, 72]             112\n",
      "           Conv2d-20           [-1, 56, 72, 72]           2,744\n",
      "         Residual-21           [-1, 56, 72, 72]               0\n",
      "        MaxPool2d-22           [-1, 56, 36, 36]               0\n",
      "           Conv2d-23           [-1, 64, 36, 36]          32,320\n",
      "      BatchNorm2d-24           [-1, 64, 36, 36]             128\n",
      "           Conv2d-25           [-1, 64, 36, 36]          36,928\n",
      "      BatchNorm2d-26           [-1, 64, 36, 36]             128\n",
      "           Conv2d-27           [-1, 64, 36, 36]           3,648\n",
      "         Residual-28           [-1, 64, 36, 36]               0\n",
      "        MaxPool2d-29           [-1, 64, 18, 18]               0\n",
      "          Flatten-30                [-1, 20736]               0\n",
      "           Linear-31                  [-1, 400]       8,294,800\n",
      "   Flatten_Module-32               [-1, 20, 20]               0\n",
      "================================================================\n",
      "Total params: 8,465,120\n",
      "Trainable params: 8,465,120\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 33.54\n",
      "Params size (MB): 32.29\n",
      "Estimated Total Size (MB): 65.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, input_size=(2,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5104b2-0f10-4445-87d2-b7eac97309cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74256f1-50bf-45cb-8e78-c9f915d7365e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "339b9497-7cb0-4793-b47e-d58ef0671c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean: 1480488.125, Global Standard Deviation: 2489774.5\n",
      "Saved normalized_input1.pt\n",
      "Saved normalized_input2.pt\n",
      "Saved normalized_input3.pt\n",
      "Saved normalized_input4.pt\n",
      "Saved normalized_input5.pt\n",
      "Saved normalized_input6.pt\n",
      "Saved normalized_input7.pt\n",
      "Saved normalized_input8.pt\n",
      "Saved normalized_input9.pt\n",
      "Saved normalized_input10.pt\n",
      "Saved normalized_input11.pt\n",
      "Saved normalized_input12.pt\n",
      "Saved normalized_input13.pt\n",
      "Saved normalized_input14.pt\n",
      "Saved normalized_input15.pt\n",
      "Saved normalized_input16.pt\n",
      "Saved normalized_input17.pt\n",
      "Saved normalized_input18.pt\n",
      "Saved normalized_input19.pt\n",
      "Saved normalized_input20.pt\n",
      "Saved normalized_input21.pt\n",
      "Saved normalized_input22.pt\n",
      "Saved normalized_input23.pt\n",
      "Saved normalized_input24.pt\n",
      "Saved normalized_input25.pt\n",
      "Saved normalized_input26.pt\n",
      "Saved normalized_input27.pt\n",
      "Saved normalized_input28.pt\n",
      "Saved normalized_input29.pt\n",
      "Saved normalized_input30.pt\n",
      "Saved normalized_input31.pt\n",
      "Saved normalized_input32.pt\n",
      "Saved normalized_input33.pt\n",
      "Saved normalized_input34.pt\n",
      "Saved normalized_input35.pt\n",
      "Saved normalized_input36.pt\n",
      "Saved normalized_input37.pt\n",
      "Saved normalized_input38.pt\n",
      "Saved normalized_input39.pt\n",
      "Saved normalized_input40.pt\n",
      "Saved normalized_input41.pt\n",
      "Saved normalized_input42.pt\n",
      "Saved normalized_input43.pt\n",
      "Saved normalized_input44.pt\n",
      "Saved normalized_input45.pt\n",
      "Saved normalized_input46.pt\n",
      "Saved normalized_input47.pt\n",
      "Saved normalized_input48.pt\n",
      "Saved normalized_input49.pt\n",
      "Saved normalized_input50.pt\n",
      "Saved normalized_input51.pt\n",
      "Saved normalized_input52.pt\n",
      "Saved normalized_input53.pt\n",
      "Saved normalized_input54.pt\n",
      "Saved normalized_input55.pt\n",
      "Saved normalized_input56.pt\n",
      "Saved normalized_input57.pt\n",
      "Saved normalized_input58.pt\n",
      "Saved normalized_input59.pt\n",
      "Saved normalized_input60.pt\n",
      "Saved normalized_input61.pt\n",
      "Saved normalized_input62.pt\n",
      "Saved normalized_input63.pt\n",
      "Saved normalized_input64.pt\n",
      "Saved normalized_input65.pt\n",
      "Saved normalized_input66.pt\n",
      "Saved normalized_input67.pt\n",
      "Saved normalized_input68.pt\n",
      "Saved normalized_input69.pt\n",
      "Saved normalized_input70.pt\n",
      "Saved normalized_input71.pt\n",
      "Saved normalized_input72.pt\n",
      "Saved normalized_input73.pt\n",
      "Saved normalized_input74.pt\n",
      "Saved normalized_input75.pt\n",
      "Saved normalized_input76.pt\n",
      "Saved normalized_input77.pt\n",
      "Saved normalized_input78.pt\n",
      "Saved normalized_input79.pt\n",
      "Saved normalized_input80.pt\n",
      "Saved normalized_input81.pt\n",
      "Saved normalized_input82.pt\n",
      "Saved normalized_input83.pt\n",
      "Saved normalized_input84.pt\n",
      "Saved normalized_input85.pt\n",
      "Saved normalized_input86.pt\n",
      "Saved normalized_input87.pt\n",
      "Saved normalized_input88.pt\n",
      "Saved normalized_input89.pt\n",
      "Saved normalized_input90.pt\n",
      "Saved normalized_input91.pt\n",
      "Saved normalized_input92.pt\n",
      "Saved normalized_input93.pt\n",
      "Saved normalized_input94.pt\n",
      "Saved normalized_input95.pt\n",
      "Saved normalized_input96.pt\n",
      "Saved normalized_input97.pt\n",
      "Saved normalized_input98.pt\n",
      "Saved normalized_input99.pt\n",
      "Saved normalized_input100.pt\n",
      "Saved normalized_input101.pt\n",
      "Saved normalized_input102.pt\n",
      "Saved normalized_input103.pt\n",
      "Saved normalized_input104.pt\n",
      "Saved normalized_input105.pt\n",
      "Saved normalized_input106.pt\n",
      "Saved normalized_input107.pt\n",
      "Saved normalized_input108.pt\n",
      "Saved normalized_input109.pt\n",
      "Saved normalized_input110.pt\n",
      "Saved normalized_input111.pt\n",
      "Saved normalized_input112.pt\n",
      "Saved normalized_input113.pt\n",
      "Saved normalized_input114.pt\n",
      "Saved normalized_input115.pt\n",
      "Saved normalized_input116.pt\n",
      "Saved normalized_input117.pt\n",
      "Saved normalized_input118.pt\n",
      "Saved normalized_input119.pt\n",
      "Saved normalized_input120.pt\n",
      "Saved normalized_input121.pt\n",
      "Saved normalized_input122.pt\n",
      "Saved normalized_input123.pt\n",
      "Saved normalized_input124.pt\n",
      "Saved normalized_input125.pt\n",
      "Saved normalized_input126.pt\n",
      "Saved normalized_input127.pt\n",
      "Saved normalized_input128.pt\n",
      "Saved normalized_input129.pt\n",
      "Saved normalized_input130.pt\n",
      "Saved normalized_input131.pt\n",
      "Saved normalized_input132.pt\n",
      "Saved normalized_input133.pt\n",
      "Saved normalized_input134.pt\n",
      "Saved normalized_input135.pt\n",
      "Saved normalized_input136.pt\n",
      "Saved normalized_input137.pt\n",
      "Saved normalized_input138.pt\n",
      "Saved normalized_input139.pt\n",
      "Saved normalized_input140.pt\n",
      "Saved normalized_input141.pt\n",
      "Saved normalized_input142.pt\n",
      "Saved normalized_input143.pt\n",
      "Saved normalized_input144.pt\n",
      "Saved normalized_input145.pt\n",
      "Saved normalized_input146.pt\n",
      "Saved normalized_input147.pt\n",
      "Saved normalized_input148.pt\n",
      "Saved normalized_input149.pt\n",
      "Saved normalized_input150.pt\n",
      "Saved normalized_input151.pt\n",
      "Saved normalized_input152.pt\n",
      "Saved normalized_input153.pt\n",
      "Saved normalized_input154.pt\n",
      "Saved normalized_input155.pt\n",
      "Saved normalized_input156.pt\n",
      "Saved normalized_input157.pt\n",
      "Saved normalized_input158.pt\n",
      "Saved normalized_input159.pt\n",
      "Saved normalized_input160.pt\n",
      "Saved normalized_input161.pt\n",
      "Saved normalized_input162.pt\n",
      "Saved normalized_input163.pt\n",
      "Saved normalized_input164.pt\n",
      "Saved normalized_input165.pt\n",
      "Saved normalized_input166.pt\n",
      "Saved normalized_input167.pt\n",
      "Saved normalized_input168.pt\n",
      "Saved normalized_input169.pt\n",
      "Saved normalized_input170.pt\n",
      "Saved normalized_input171.pt\n",
      "Saved normalized_input172.pt\n",
      "Saved normalized_input173.pt\n",
      "Saved normalized_input174.pt\n",
      "Saved normalized_input175.pt\n",
      "Saved normalized_input176.pt\n",
      "Saved normalized_input177.pt\n",
      "Saved normalized_input178.pt\n",
      "Saved normalized_input179.pt\n",
      "Saved normalized_input180.pt\n",
      "Saved normalized_input181.pt\n",
      "Saved normalized_input182.pt\n",
      "Saved normalized_input183.pt\n",
      "Saved normalized_input184.pt\n",
      "Saved normalized_input185.pt\n",
      "Saved normalized_input186.pt\n",
      "Saved normalized_input187.pt\n",
      "Saved normalized_input188.pt\n",
      "Saved normalized_input189.pt\n",
      "Saved normalized_input190.pt\n",
      "Saved normalized_input191.pt\n",
      "Saved normalized_input192.pt\n",
      "Saved normalized_input193.pt\n",
      "Saved normalized_input194.pt\n",
      "Saved normalized_input195.pt\n",
      "Saved normalized_input196.pt\n",
      "Saved normalized_input197.pt\n",
      "Saved normalized_input198.pt\n",
      "Saved normalized_input199.pt\n",
      "Saved normalized_input200.pt\n",
      "Saved normalized_input201.pt\n",
      "Saved normalized_input202.pt\n",
      "Saved normalized_input203.pt\n",
      "Saved normalized_input204.pt\n",
      "Saved normalized_input205.pt\n",
      "Saved normalized_input206.pt\n",
      "Saved normalized_input207.pt\n",
      "Saved normalized_input208.pt\n",
      "Saved normalized_input209.pt\n",
      "Saved normalized_input210.pt\n",
      "Saved normalized_input211.pt\n",
      "Saved normalized_input212.pt\n",
      "Saved normalized_input213.pt\n",
      "Saved normalized_input214.pt\n",
      "Saved normalized_input215.pt\n",
      "Saved normalized_input216.pt\n",
      "Saved normalized_input217.pt\n",
      "Saved normalized_input218.pt\n",
      "Saved normalized_input219.pt\n",
      "Saved normalized_input220.pt\n",
      "Saved normalized_input221.pt\n",
      "Saved normalized_input222.pt\n",
      "Saved normalized_input223.pt\n",
      "Saved normalized_input224.pt\n",
      "Saved normalized_input225.pt\n",
      "Saved normalized_input226.pt\n",
      "Saved normalized_input227.pt\n",
      "Saved normalized_input228.pt\n",
      "Saved normalized_input229.pt\n",
      "Saved normalized_input230.pt\n",
      "Saved normalized_input231.pt\n",
      "Saved normalized_input232.pt\n",
      "Saved normalized_input233.pt\n",
      "Saved normalized_input234.pt\n",
      "Saved normalized_input235.pt\n",
      "Saved normalized_input236.pt\n",
      "Saved normalized_input237.pt\n",
      "Saved normalized_input238.pt\n",
      "Saved normalized_input239.pt\n",
      "Saved normalized_input240.pt\n",
      "Saved normalized_input241.pt\n",
      "Saved normalized_input242.pt\n",
      "Saved normalized_input243.pt\n",
      "Saved normalized_input244.pt\n",
      "Saved normalized_input245.pt\n",
      "Saved normalized_input246.pt\n",
      "Saved normalized_input247.pt\n",
      "Saved normalized_input248.pt\n",
      "Saved normalized_input249.pt\n",
      "Saved normalized_input250.pt\n",
      "Saved normalized_input251.pt\n",
      "Saved normalized_input252.pt\n",
      "Saved normalized_input253.pt\n",
      "Saved normalized_input254.pt\n",
      "Saved normalized_input255.pt\n",
      "Saved normalized_input256.pt\n",
      "Saved normalized_input257.pt\n",
      "Saved normalized_input258.pt\n",
      "Saved normalized_input259.pt\n",
      "Saved normalized_input260.pt\n",
      "Saved normalized_input261.pt\n",
      "Saved normalized_input262.pt\n",
      "Saved normalized_input263.pt\n",
      "Saved normalized_input264.pt\n",
      "Saved normalized_input265.pt\n",
      "Saved normalized_input266.pt\n",
      "Saved normalized_input267.pt\n",
      "Saved normalized_input268.pt\n",
      "Saved normalized_input269.pt\n",
      "Saved normalized_input270.pt\n",
      "Saved normalized_input271.pt\n",
      "Saved normalized_input272.pt\n",
      "Saved normalized_input273.pt\n",
      "Saved normalized_input274.pt\n",
      "Saved normalized_input275.pt\n",
      "Saved normalized_input276.pt\n",
      "Saved normalized_input277.pt\n",
      "Saved normalized_input278.pt\n",
      "Saved normalized_input279.pt\n",
      "Saved normalized_input280.pt\n",
      "Saved normalized_input281.pt\n",
      "Saved normalized_input282.pt\n",
      "Saved normalized_input283.pt\n",
      "Saved normalized_input284.pt\n",
      "Saved normalized_input285.pt\n",
      "Saved normalized_input286.pt\n",
      "Saved normalized_input287.pt\n",
      "Saved normalized_input288.pt\n",
      "Saved normalized_input289.pt\n",
      "Saved normalized_input290.pt\n",
      "Saved normalized_input291.pt\n",
      "Saved normalized_input292.pt\n",
      "Saved normalized_input293.pt\n",
      "Saved normalized_input294.pt\n",
      "Saved normalized_input295.pt\n",
      "Saved normalized_input296.pt\n",
      "Saved normalized_input297.pt\n",
      "Saved normalized_input298.pt\n",
      "Saved normalized_input299.pt\n",
      "Saved normalized_input300.pt\n",
      "Saved normalized_input301.pt\n",
      "Saved normalized_input302.pt\n",
      "Saved normalized_input303.pt\n",
      "Saved normalized_input304.pt\n",
      "Saved normalized_input305.pt\n",
      "Saved normalized_input306.pt\n",
      "Saved normalized_input307.pt\n",
      "Saved normalized_input308.pt\n",
      "Saved normalized_input309.pt\n",
      "Saved normalized_input310.pt\n",
      "Saved normalized_input311.pt\n",
      "Saved normalized_input312.pt\n",
      "Saved normalized_input313.pt\n",
      "Saved normalized_input314.pt\n",
      "Saved normalized_input315.pt\n",
      "Saved normalized_input316.pt\n",
      "Saved normalized_input317.pt\n",
      "Saved normalized_input318.pt\n",
      "Saved normalized_input319.pt\n",
      "Saved normalized_input320.pt\n",
      "Saved normalized_input321.pt\n",
      "Saved normalized_input322.pt\n",
      "Saved normalized_input323.pt\n",
      "Saved normalized_input324.pt\n",
      "Saved normalized_input325.pt\n",
      "Saved normalized_input326.pt\n",
      "Saved normalized_input327.pt\n",
      "Saved normalized_input328.pt\n",
      "Saved normalized_input329.pt\n",
      "Saved normalized_input330.pt\n",
      "Saved normalized_input331.pt\n",
      "Saved normalized_input332.pt\n",
      "Saved normalized_input333.pt\n",
      "Saved normalized_input334.pt\n",
      "Saved normalized_input335.pt\n",
      "Saved normalized_input336.pt\n",
      "Saved normalized_input337.pt\n",
      "Saved normalized_input338.pt\n",
      "Saved normalized_input339.pt\n",
      "Saved normalized_input340.pt\n",
      "Saved normalized_input341.pt\n",
      "Saved normalized_input342.pt\n",
      "Saved normalized_input343.pt\n",
      "Saved normalized_input344.pt\n",
      "Saved normalized_input345.pt\n",
      "Saved normalized_input346.pt\n",
      "Saved normalized_input347.pt\n",
      "Saved normalized_input348.pt\n",
      "Saved normalized_input349.pt\n",
      "Saved normalized_input350.pt\n",
      "Saved normalized_input351.pt\n",
      "Saved normalized_input352.pt\n",
      "Saved normalized_input353.pt\n",
      "Saved normalized_input354.pt\n",
      "Saved normalized_input355.pt\n",
      "Saved normalized_input356.pt\n",
      "Saved normalized_input357.pt\n",
      "Saved normalized_input358.pt\n",
      "Saved normalized_input359.pt\n",
      "Saved normalized_input360.pt\n",
      "Saved normalized_input361.pt\n",
      "Saved normalized_input362.pt\n",
      "Saved normalized_input363.pt\n",
      "Saved normalized_input364.pt\n",
      "Saved normalized_input365.pt\n",
      "Saved normalized_input366.pt\n",
      "Saved normalized_input367.pt\n",
      "Saved normalized_input368.pt\n",
      "Saved normalized_input369.pt\n",
      "Saved normalized_input370.pt\n",
      "Saved normalized_input371.pt\n",
      "Saved normalized_input372.pt\n",
      "Saved normalized_input373.pt\n",
      "Saved normalized_input374.pt\n",
      "Saved normalized_input375.pt\n",
      "Saved normalized_input376.pt\n",
      "Saved normalized_input377.pt\n",
      "Saved normalized_input378.pt\n",
      "Saved normalized_input379.pt\n",
      "Saved normalized_input380.pt\n",
      "Saved normalized_input381.pt\n",
      "Saved normalized_input382.pt\n",
      "Saved normalized_input383.pt\n",
      "Saved normalized_input384.pt\n",
      "Saved normalized_input385.pt\n",
      "Saved normalized_input386.pt\n",
      "Saved normalized_input387.pt\n",
      "Saved normalized_input388.pt\n",
      "Saved normalized_input389.pt\n",
      "Saved normalized_input390.pt\n",
      "Saved normalized_input391.pt\n",
      "Saved normalized_input392.pt\n",
      "Saved normalized_input393.pt\n",
      "Saved normalized_input394.pt\n",
      "Saved normalized_input395.pt\n",
      "Saved normalized_input396.pt\n",
      "Saved normalized_input397.pt\n",
      "Saved normalized_input398.pt\n",
      "Saved normalized_input399.pt\n",
      "Saved normalized_input400.pt\n",
      "Saved normalized_input401.pt\n",
      "Saved normalized_input402.pt\n",
      "Saved normalized_input403.pt\n",
      "Saved normalized_input404.pt\n",
      "Saved normalized_input405.pt\n",
      "Saved normalized_input406.pt\n",
      "Saved normalized_input407.pt\n",
      "Saved normalized_input408.pt\n",
      "Saved normalized_input409.pt\n",
      "Saved normalized_input410.pt\n",
      "Saved normalized_input411.pt\n",
      "Saved normalized_input412.pt\n",
      "Saved normalized_input413.pt\n",
      "Saved normalized_input414.pt\n",
      "Saved normalized_input415.pt\n",
      "Saved normalized_input416.pt\n",
      "Saved normalized_input417.pt\n",
      "Saved normalized_input418.pt\n",
      "Saved normalized_input419.pt\n",
      "Saved normalized_input420.pt\n",
      "Saved normalized_input421.pt\n",
      "Saved normalized_input422.pt\n",
      "Saved normalized_input423.pt\n",
      "Saved normalized_input424.pt\n",
      "Saved normalized_input425.pt\n",
      "Saved normalized_input426.pt\n",
      "Saved normalized_input427.pt\n",
      "Saved normalized_input428.pt\n",
      "Saved normalized_input429.pt\n",
      "Saved normalized_input430.pt\n",
      "Saved normalized_input431.pt\n",
      "Saved normalized_input432.pt\n",
      "Saved normalized_input433.pt\n",
      "Saved normalized_input434.pt\n",
      "Saved normalized_input435.pt\n",
      "Saved normalized_input436.pt\n",
      "Saved normalized_input437.pt\n",
      "Saved normalized_input438.pt\n",
      "Saved normalized_input439.pt\n",
      "Saved normalized_input440.pt\n",
      "Saved normalized_input441.pt\n",
      "Saved normalized_input442.pt\n",
      "Saved normalized_input443.pt\n",
      "Saved normalized_input444.pt\n",
      "Saved normalized_input445.pt\n",
      "Saved normalized_input446.pt\n",
      "Saved normalized_input447.pt\n",
      "Saved normalized_input448.pt\n",
      "Saved normalized_input449.pt\n",
      "Saved normalized_input450.pt\n",
      "Saved normalized_input451.pt\n",
      "Saved normalized_input452.pt\n",
      "Saved normalized_input453.pt\n",
      "Saved normalized_input454.pt\n",
      "Saved normalized_input455.pt\n",
      "Saved normalized_input456.pt\n",
      "Saved normalized_input457.pt\n",
      "Saved normalized_input458.pt\n",
      "Saved normalized_input459.pt\n",
      "Saved normalized_input460.pt\n",
      "Saved normalized_input461.pt\n",
      "Saved normalized_input462.pt\n",
      "Saved normalized_input463.pt\n",
      "Saved normalized_input464.pt\n",
      "Saved normalized_input465.pt\n",
      "Saved normalized_input466.pt\n",
      "Saved normalized_input467.pt\n",
      "Saved normalized_input468.pt\n",
      "Saved normalized_input469.pt\n",
      "Saved normalized_input470.pt\n",
      "Saved normalized_input471.pt\n",
      "Saved normalized_input472.pt\n",
      "Saved normalized_input473.pt\n",
      "Saved normalized_input474.pt\n",
      "Saved normalized_input475.pt\n",
      "Saved normalized_input476.pt\n",
      "Saved normalized_input477.pt\n",
      "Saved normalized_input478.pt\n",
      "Saved normalized_input479.pt\n",
      "Saved normalized_input480.pt\n",
      "Saved normalized_input481.pt\n",
      "Saved normalized_input482.pt\n",
      "Saved normalized_input483.pt\n",
      "Saved normalized_input484.pt\n",
      "Saved normalized_input485.pt\n",
      "Saved normalized_input486.pt\n",
      "Saved normalized_input487.pt\n",
      "Saved normalized_input488.pt\n",
      "Saved normalized_input489.pt\n",
      "Saved normalized_input490.pt\n",
      "Saved normalized_input491.pt\n",
      "Saved normalized_input492.pt\n",
      "Saved normalized_input493.pt\n",
      "Saved normalized_input494.pt\n",
      "Saved normalized_input495.pt\n",
      "Saved normalized_input496.pt\n",
      "Saved normalized_input497.pt\n",
      "Saved normalized_input498.pt\n",
      "Saved normalized_input499.pt\n",
      "Saved normalized_input500.pt\n",
      "Saved normalized_input501.pt\n",
      "Saved normalized_input502.pt\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个空列表来存储所有张量\n",
    "all_raw_target = []\n",
    "\n",
    "for i in range(502):\n",
    "    raw_target = torch.load(os.path.join(r\"C:\\Users\\ROG\\test_v3\\5500\", f'input-{i+1}.pt'))\n",
    "\n",
    "    all_raw_target.append(raw_target)\n",
    "\n",
    "\n",
    "all_raw_target_stack= torch.stack(all_raw_target) \n",
    "all_raw_target_stack.shape\n",
    "\n",
    "# 计算全局均值和标准差\n",
    "target_mean = all_raw_target_stack.mean()\n",
    "target_std = all_raw_target_stack.std()\n",
    "print(f\"Global Mean: {target_mean.item()}, Global Standard Deviation: {target_std.item()}\")\n",
    "\n",
    "# 对每个张量进行 Z-score 归一化\n",
    "target_normalized_tensors = (all_raw_target_stack - target_mean) / target_std \n",
    "\n",
    "for i in range(target_normalized_tensors.shape[0]):  # 300 个样本\n",
    "    # 提取第 i 个 (20, 20) 的张量\n",
    "    tensor_i = target_normalized_tensors[i]\n",
    "    \n",
    "    # 构建保存路径\n",
    "    output_file = os.path.join(r\"C:\\Users\\ROG\\test_v3\\5500_normalized\", f'normalized_input_{i+1}.pt')\n",
    "    \n",
    "    # 保存张量\n",
    "    torch.save(tensor_i, output_file)\n",
    "    print(f\"Saved normalized_input{i+1}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31560373-8061-4fc8-b958-10811989e382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3911,  1.3906],\n",
       "         [ 1.3897,  1.3889]],\n",
       "\n",
       "        [[-0.5924, -0.5946],\n",
       "         [-0.5924, -0.5946]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(r\"C:\\Users\\ROG\\test_v3\\5500_normalized\\normalized_input_278.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85cf5c84-f272-48d7-aa64-8ceb4be30ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.9439e+06, 4.9428e+06],\n",
       "         [4.9404e+06, 4.9386e+06]],\n",
       "\n",
       "        [[5.5000e+03, 3.0843e-01],\n",
       "         [5.5000e+03, 3.0843e-01]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(r\"C:\\Users\\ROG\\test_v3\\5500\\input-278.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d286b-d840-4098-9059-4034657a3f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a78223-365c-4186-bb7d-73de17ea60d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116e7a1-b6e6-4146-b527-e92beb6e8506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd2a59-5a41-4e7b-a1e9-908bfc542386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101abbe7-fcb3-44f4-abe1-8383ce549608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09e3d2-8f01-4e07-ab3c-275fe03825e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91180d14-45b3-4519-9d1f-edb467c12498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
